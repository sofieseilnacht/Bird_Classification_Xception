{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LkAt4xhuz2TP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras import callbacks\n",
        "import keras\n",
        "import keras_tuner\n",
        "import keras.utils as image\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras import callbacks\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49L3SjdWR8Oa"
      },
      "source": [
        "## Importing Data from csv files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6Q84PvEJobQu",
        "outputId": "bc83029b-8453-4423-d0da-4b8bab6425dd"
      },
      "outputs": [],
      "source": [
        "images = pd.read_csv(\"/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/images.txt\", sep=r'\\s+', names=['image_id', 'image_name'], engine='python')\n",
        "train_test_split = pd.read_csv(\"/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/train_test_split.txt\", sep=r'\\s+', names=['image_id', 'is_training_image'], engine='python')\n",
        "classes =pd.read_csv(\"/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/classes.txt\", sep=r'\\s+', names=['class_id', 'class_name'], engine='python')\n",
        "image_class_labels =pd.read_csv(\"/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/image_class_labels.txt\", sep=r'\\s+', names=['image_id', 'class_id'], engine='python')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay9R_UOcSKkH"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rj27i5GRKG1",
        "outputId": "74981fbd-a7de-43a8-8ffd-76dab7ec4c11"
      },
      "outputs": [],
      "source": [
        "# Merge dfs based on column names so we have one df with all the necessary info contained per each row\n",
        "image_data = pd.merge(images,train_test_split, on='image_id')\n",
        "image_data = pd.merge(image_data,image_class_labels, on='image_id')\n",
        "image_data = pd.merge(image_data,classes, on='class_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split training and testing image data\n",
        "training_image_data = image_data[image_data['is_training_image']==1]\n",
        "testing_image_data = image_data[image_data['is_training_image']==0]\n",
        "\n",
        "# Shuffle training data\n",
        "training_image_data = training_image_data.sample(frac=1)\n",
        "\n",
        "# Initiate empty lists for training and testing images\n",
        "training_images = []\n",
        "testing_images = []\n",
        "\n",
        "# Add training and testing images to corresponding lists\n",
        "for i in (training_image_data['image_name'].values):\n",
        "    training_images.append(image.load_img('/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/images/{}'.format(i), target_size=(224, 224)))\n",
        "\n",
        "for i in (testing_image_data['image_name'].values):\n",
        "    testing_images.append(image.load_img('/Users/sofie/Desktop/Projects/Classification of Birds/CUB_200_2011/images/{}'.format(i), target_size=(224, 224)))\n",
        "\n",
        "# Extract class labels for training and testing images\n",
        "training_class_label = np.array(training_image_data['class_id'].values)\n",
        "testing_class_label = np.array(testing_image_data['class_id'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert list of images to NumPy array\n",
        "training_images = np.array(training_images)\n",
        "testing_images = np.array(testing_images)\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_training_images = preprocess_input(training_images)\n",
        "preprocessed_testing_images = preprocess_input(testing_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin training by keeping the existing layers frozen, allowing only the newly added classification head to learn. This ensures that the output layer is properly trained before fine-tuning the deeper layers. Once the classifier stabilizes, we gradually unfreeze and fine-tune the earlier layers, optimizing them in a controlled manner. This approach prevents instability in feature extraction and allows each layer to adjust effectively, improving overall model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.0801 - loss: 4.8940 - val_accuracy: 0.3235 - val_loss: 2.8770\n",
            "Epoch 2/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.4155 - loss: 2.4539 - val_accuracy: 0.4197 - val_loss: 2.3716\n",
            "Epoch 3/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.5498 - loss: 1.8479 - val_accuracy: 0.4486 - val_loss: 2.1636\n",
            "Epoch 4/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 1s/step - accuracy: 0.6324 - loss: 1.5001 - val_accuracy: 0.4608 - val_loss: 2.0738\n",
            "Epoch 5/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.6954 - loss: 1.2807 - val_accuracy: 0.4903 - val_loss: 2.0099\n",
            "Epoch 6/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.7477 - loss: 1.0901 - val_accuracy: 0.4942 - val_loss: 1.9443\n",
            "Epoch 7/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.7768 - loss: 0.9528 - val_accuracy: 0.4975 - val_loss: 1.9045\n",
            "Epoch 8/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.7866 - loss: 0.8790 - val_accuracy: 0.5069 - val_loss: 1.9052\n",
            "Epoch 9/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.8227 - loss: 0.7638 - val_accuracy: 0.5025 - val_loss: 1.8916\n",
            "Epoch 10/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.8381 - loss: 0.7179 - val_accuracy: 0.5103 - val_loss: 1.8793\n",
            "Epoch 11/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.8540 - loss: 0.6486 - val_accuracy: 0.5025 - val_loss: 1.8887\n",
            "Epoch 12/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.8771 - loss: 0.5670 - val_accuracy: 0.5131 - val_loss: 1.8811\n",
            "Epoch 13/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.8846 - loss: 0.5361 - val_accuracy: 0.5114 - val_loss: 1.8920\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained Xception model (without top layers)\n",
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "# Add new layers on top\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "dense = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(l2=1e-4))(avg)  # L2 regularization\n",
        "dropout = keras.layers.Dropout(0.5, noise_shape=None, seed=None)(dense) \n",
        "output = keras.layers.Dense(201, activation=\"softmax\")(dropout)  # 200 classes\n",
        "model = keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Use Adam with a fixed learning rate\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Set early stopping\n",
        "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define validation data split at 30%\n",
        "split_idx = int(len(preprocessed_training_images) * 0.7)\n",
        "X_train, X_val = preprocessed_training_images[:split_idx], preprocessed_training_images[split_idx:]\n",
        "y_train, y_val = training_class_label[:split_idx], training_class_label[split_idx:]\n",
        "\n",
        "y_train = np.array(y_train).astype(np.int32)\n",
        "y_val = np.array(y_val).astype(np.int32)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32, callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 725ms/step - accuracy: 0.5444 - loss: 1.7000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.8986423015594482, 0.5053503513336182]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(preprocessed_testing_images, testing_class_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
